We'll be interacting via the HTTP APIs for getting metadata and particularly dealing with the `labelmap`
datatype. Each datatype has a separate HTTP API defined in the `datatype/<datatype>/<datatype>.go` file as a large text string set to `helpMessage`. We are
interested in the `labelmap` datatype so look at the const `helpMessage` set in `datatype/labelmap/labelmap.go`.

Note that all calls will require a UUID that gives the "version" of the data as well as the string for a DVID server, so HTTP requests will be of the form:
"http://<server>/api/node/<UUID>/<data instance name>/..."

Usually the <data instance name> will be "segmentation" since we are getting label data organized as smaller 3d 64x64x64 chunks that tile a much larger 3d space.

1. Given a neuron uint64 (the label or body ID), we will get a LabelIndex defined by a protobuf serialization via the /api/node/<UUID>/segmentation/index/<label
   ID>.

2. The LabeIndex will provide a map of block coordinates (each block being a 64x64x64 ZYX array of uint64 labels) mapped to a SVCount protobuf message. Each
   block's SVCount will list provide a mapping of supervoxel IDs to the number of voxels with that ID in the block. Each neuron, which is designated by a label ID,
   could contain any number of supervoxels (each with a separate supervoxel ID). But all we really care about is for each block, how many voxels really belong to
   the given label ID, which is the sum of all the counts in SVCount.

3. Now that we have for the given label the distribution of blocks (through the block coordinates) and for each block the number of voxels belonging to the
   label, we can determine how many sample points will be in each block from the sample density parameter (SD = 0.0001 to 1.0). From the LabelIndex we know the
   number of voxels, N, for that label, so we can compute how many sample points as SP = N * SD rounded to nearest integer. We'll then randomly select points by
   having an array of SP indices (integers) that uniquely specify which of our N voxels are selected among the 0..N-1 points.

4. We'll then traverse each block in SVCount by block coordinate, read it from the DVID server using the `/api/node/<UUID>/segmentation/sparsevol/<label ID>`
   endpoint, maybe starting with "rles" format. As we read block after block, we'll keep track of the number of voxels we've traverse across all blocks as well as
   iterate through the array of sample point indices. If sample point indices match the index of voxels within the current block, we calculate the point coordinate
   from the ZYX block coordinate and factor in each block is 64 x 64 x 64 voxels. This gives us a 3d point coordinate for each chosen sample.

   Ideally, the interface to this package would look like:
   ```python
   import dvid_point_cloud as dpc

   server = "http://my-dvid-server.janelia.org"
   uuid = "bc9a0f"  # The version is a hexadecimal string
   label_id = 189310
   density = 0.01
   point_cloud = dpc.uniform_sample(server, uuid, label_id, density)
   ```

   The `point_cloud` return value should be a numpy array of shape (SP, 3) corresponding to the XYZ point (voxel) coordinates. There will eventually be other
   sampling methods aside from uniformly sampling across a sparse volume, e.g., sampling from only the surface of a sparse volume.
